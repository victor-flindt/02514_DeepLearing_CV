{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DRIhx7PugJy3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ic_gOv_pUZeB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code will run on GPU.\n",
      "Tesla V100-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"The code will run on GPU.\")\n",
    "else:\n",
    "    print(\"The code will run on CPU. Go to Edit->Notebook Settings and choose GPU as the hardware accelerator\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.get_device_name(0))\n",
    "# print(torch.cuda.memory_usage(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = '/dtu/datasets1/02514/data_wastedetection'\n",
    "anns_file_path = ROOT_PATH + '/' + 'annotations.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of super categories: 28\n",
      "Number of categories: 60\n",
      "Number of annotations: 4784\n",
      "Number of images: 1500\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Read annotations\n",
    "\n",
    "with open(anns_file_path, 'r') as f:\n",
    "    dataset = json.loads(f.read())\n",
    "\n",
    "categories = dataset['categories']\n",
    "anns = dataset['annotations']\n",
    "imgs = dataset['images']\n",
    "nr_cats = len(categories)\n",
    "nr_annotations = len(anns)\n",
    "nr_images = len(imgs)\n",
    "\n",
    "# Load categories and super categories\n",
    "cat_names = []\n",
    "super_cat_names = []\n",
    "super_cat_ids = {}\n",
    "super_cat_last_name = ''\n",
    "nr_super_cats = 0\n",
    "for cat_it in categories:\n",
    "    cat_names.append(cat_it['name'])\n",
    "    super_cat_name = cat_it['supercategory']\n",
    "    # Adding new supercat\n",
    "    if super_cat_name != super_cat_last_name:\n",
    "        super_cat_names.append(super_cat_name)\n",
    "        super_cat_ids[super_cat_name] = nr_super_cats\n",
    "        super_cat_last_name = super_cat_name\n",
    "        nr_super_cats += 1\n",
    "super_cat_names = [\"background\"]+super_cat_names\n",
    "\n",
    "print('Number of super categories:', nr_super_cats)\n",
    "print('Number of categories:', nr_cats)\n",
    "print('Number of annotations:', nr_annotations)\n",
    "print('Number of images:', nr_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linking the regulear categories to super categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_ids_2_supercat_ids = {}\n",
    "for cat in categories:\n",
    "    cat_ids_2_supercat_ids[cat['id']+1] = super_cat_ids[cat['supercategory']]+1\n",
    "    \n",
    "class label(object):\n",
    "    def __init__(self,cat_ids_2_supercat_ids):\n",
    "        self.cat_ids_2_super_cat_ids = cat_ids_2_supercat_ids\n",
    "    def __call__(self,sample):\n",
    "        for i in range(len(sample)):\n",
    "            sample[i][\"category_id\"] = cat_ids_2_supercat_ids[sample[i][\"category_id\"]]\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.Resize((500, 500)),\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(data, ss,x_scale,y_scale):\n",
    "    gt = []\n",
    "    inside_gt = []\n",
    "    background = []\n",
    "    \n",
    "    for objects in range(len(data)):\n",
    "        gt.append(data[objects][\"bbox\"])\n",
    "        x11 = gt[objects][0]*x_scale\n",
    "        y11 = gt[objects][1]*y_scale\n",
    "        x12 = (gt[objects][2]*x_scale)+x11\n",
    "        y12 = (gt[objects][3]*y_scale)+y11\n",
    "        add_background = False\n",
    "        for index,candidates in enumerate(ss):\n",
    "\n",
    "            x21 = ss[index][0]\n",
    "            y21 = ss[index][1]\n",
    "            x22 = ss[index][2]+x21\n",
    "            y22 = ss[index][3]+y21\n",
    "            \n",
    "            box1 = torch.tensor([[x11, y11, x12, y12]], dtype=torch.float)\n",
    "            box2 = torch.tensor([[x21, y21, x22, y22]], dtype=torch.float)\n",
    "            iou = float(bops.box_iou(box1, box2).cpu())\n",
    "            if iou > 0.5:\n",
    "                inside_gt.append([[x21,y21,x22-x21,y22-y21],data[objects][\"category_id\"]])\n",
    "                add_background = True\n",
    "                \n",
    "            if (add_background == True) & (iou < 0.20):\n",
    "                background.append([[x21,y21,x22-x21,y22-y21],0])\n",
    "                add_background = False\n",
    "                \n",
    "                \n",
    "    return inside_gt+background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDetection(torch.utils.data.Dataset):\n",
    "    \"\"\"`MS Coco Detection <http://mscoco.org/dataset/#detections-challenge2016>`_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory where images are downloaded to.\n",
    "        annFile (string): Path to json annotation file.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.ToTensor``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, annFile, transform=None, target_transform=None):\n",
    "        from pycocotools.coco import COCO\n",
    "        self.root = root\n",
    "        self.coco = COCO(annFile)\n",
    "        self.ids = list(self.coco.imgs.keys())\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tuple (image, target). target is the object returned by ``coco.loadAnns``.\n",
    "        \"\"\"\n",
    "        coco = self.coco\n",
    "        self.img_id = self.ids[index]\n",
    "        ann_ids = coco.getAnnIds(imgIds=self.img_id)\n",
    "        target = coco.loadAnns(ann_ids)\n",
    "\n",
    "        path = coco.loadImgs(self.img_id)[0]['file_name']\n",
    "\n",
    "        img = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            \n",
    "            target = self.target_transform(target)\n",
    "        \n",
    "        return img, target\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += 'Number of datapoints: {}\\n'.format(self.__len__())\n",
    "        fmt_str += 'Root Location: {}\\n'.format(self.root)\n",
    "        tmp = '    Transforms (if any): '\n",
    "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        tmp = '    Target Transforms (if any): '\n",
    "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        return fmt_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.14s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "coco = COCO(anns_file_path)\n",
    "\n",
    "dataset = CocoDetection(ROOT_PATH,anns_file_path,target_transform=label(cat_ids_2_supercat_ids),transform=train_transform)\n",
    "\n",
    "trainsize = int(0.8*len(dataset))\n",
    "testsize = len(dataset)-trainsize\n",
    "\n",
    "train_data,test_data = torch.utils.data.random_split(dataset,[trainsize,testsize])\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=3,collate_fn=lambda x: x)\n",
    "test_loader  = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=3,collate_fn=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle\n",
    "import torchvision.ops.boxes as bops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plt.imshow(image)\n",
    "\n",
    "#for index,rect in enumerate(inside):\n",
    "#    plt.gca().add_patch(Rectangle((rect[0],rect[1]),rect[2],rect[3],\n",
    "#                        edgecolor='red',\n",
    "#                        facecolor='none',\n",
    "#                        lw=1))\n",
    "#    for i in range(len(data)):\n",
    "#        plt.gca().add_patch(Rectangle((data[i][\"bbox\"][0]*x_scale,data[i][\"bbox\"][1]*y_scale),data[i][\"bbox\"][2]*x_scale,data[i][\"bbox\"][3]*y_scale,\n",
    "#                            edgecolor='blue',\n",
    "#                            facecolor='none',\n",
    "#                            lw=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating model and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "coco = COCO(anns_file_path)\n",
    "\n",
    "dataset = CocoDetection(ROOT_PATH,anns_file_path,target_transform=label(cat_ids_2_supercat_ids),transform=train_transform)\n",
    "\n",
    "trainsize = int(0.8*len(dataset))\n",
    "testsize = len(dataset)-trainsize\n",
    "\n",
    "train_data,test_data = torch.utils.data.random_split(dataset,[trainsize,testsize])\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=3,collate_fn=lambda x: x)\n",
    "test_loader  = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=3,collate_fn=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.mobilenet_v3_large()\n",
    "\n",
    "### Changing the last layer to be 10 instead of 1000\n",
    "model.classifier[3] = nn.Linear(1280,60,bias=True)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define the training as a function so we can easily re-use it.\n",
    "def train(model, optimizer, num_epochs=1):\n",
    "    #def loss_fun(output, target):\n",
    "    #    return F.nll_loss(torch.log(output), target)\n",
    "    loss_fun = nn.CrossEntropyLoss()\n",
    "    out_dict = {'train_acc': [],\n",
    "              'test_acc': [],\n",
    "              'train_loss': [],\n",
    "              'test_loss': []}\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "  \n",
    "    for epoch in tqdm(range(num_epochs), unit='epoch'):\n",
    "        model.train()\n",
    "        #For each epoch\n",
    "        train_correct = 0\n",
    "        train_loss = []\n",
    "        crop_images = []\n",
    "        #a = np.asarray(next(iter(train_loader)))\n",
    "        \n",
    "        for minibatch_number, images in enumerate(train_loader):\n",
    "            \n",
    "            for index, (image,data) in enumerate(images):\n",
    "                ### Running selective search on image\n",
    "                np_image = np.asarray(image)\n",
    "                ss.setBaseImage(np_image)\n",
    "                ss.switchToSelectiveSearchFast()\n",
    "                rects = ss.process()\n",
    "                \n",
    "                og_height = coco.loadImgs(data[0][\"image_id\"])[0][\"height\"]\n",
    "                og_width  = coco.loadImgs(data[0][\"image_id\"])[0][\"width\"]\n",
    "                x_scale = 500/og_width\n",
    "                y_scale = 500/og_height\n",
    "                \n",
    "                crop_rects_and_labels = get_iou(data,rects,x_scale,y_scale)\n",
    "                \n",
    "                for rects_and_labels in crop_rects_and_labels:\n",
    "                    (x1,y1,w,h),(label) = rects_and_labels\n",
    "                    x2 = x1+w\n",
    "                    y2 = y1+h\n",
    "                    crop_images.append(tuple([transforms.functional.pil_to_tensor(transforms.functional.resize(image.crop((x1,y1,x2,y2)),(60,60))).to(dtype=torch.float32),label]))                                     \n",
    "            \n",
    "            ### Having enough data for a training loop   \n",
    "            #crop_loader = DataLoader(crop_images,batch_size=batch_size,shuffle=True, num_workers=3)\n",
    "            trainsize = int(0.8*len(crop_images))\n",
    "            testsize = len(crop_images)-trainsize\n",
    "\n",
    "            crop_train_data,crop_test_data = torch.utils.data.random_split(crop_images,[trainsize,testsize])\n",
    "            crop_train_loader = DataLoader(crop_train_data, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "            crop_test_loader  = DataLoader(crop_test_data, batch_size=batch_size, shuffle=True, num_workers=3)  \n",
    "          \n",
    "            model.train()\n",
    "            #For each epoch\n",
    "            train_correct = 0\n",
    "            train_loss = []\n",
    "            for minibatch_no, (data, target) in tqdm(enumerate(crop_train_loader), total=len(crop_train_loader)):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                #Zero the gradients computed for each weight\n",
    "                optimizer.zero_grad()\n",
    "                #Forward pass your image through the network\n",
    "                output = model(data)\n",
    "                #Compute the loss\n",
    "                loss = loss_fun(output, target)            \n",
    "                #Backward pass through the network\n",
    "                loss.backward()\n",
    "                #Update the weights\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss.append(loss.item())\n",
    "                #Compute how many were correctly classified\n",
    "                predicted = output.argmax(1)\n",
    "                train_correct += (target==predicted).sum().cpu().item()\n",
    "                \n",
    "            #Comput the test accuracy\n",
    "            test_loss = []\n",
    "            test_correct = 0\n",
    "            model.eval()\n",
    "            for data, target in crop_test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                with torch.no_grad():\n",
    "                    output = model(data)\n",
    "                test_loss.append(loss_fun(output, target).cpu().item())\n",
    "                predicted = output.argmax(1)\n",
    "                test_correct += (target==predicted).sum().cpu().item()\n",
    "            out_dict['train_acc'].append(train_correct/len(crop_train_data))\n",
    "            out_dict['test_acc'].append(test_correct/len(crop_test_data))\n",
    "            out_dict['train_loss'].append(np.mean(train_loss))\n",
    "            out_dict['test_loss'].append(np.mean(test_loss))\n",
    "            print(f\"Loss train: {np.mean(train_loss):.3f}\\t test: {np.mean(test_loss):.3f}\\t\",\n",
    "                  f\"Accuracy train: {out_dict['train_acc'][-1]*100:.1f}%\\t test: {out_dict['test_acc'][-1]*100:.1f}%\")\n",
    "        return out_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(),lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce664a04384d4e70bbcef9ce74631507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a245911b7f142ae9c10e23406e1ef96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train: 2.413\t test: 720.558\t Accuracy train: 38.1%\t test: 0.0%\n",
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af221ed839594f70b0a114d197693cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m crop_images,crop_train_loader, crop_test_loader\u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     64\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#Update the weights\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m#Compute how many were correctly classified\u001b[39;00m\n",
      "File \u001b[0;32m~/02514_DeepLearing_CV/dlenv1/lib/python3.9/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/02514_DeepLearing_CV/dlenv1/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/02514_DeepLearing_CV/dlenv1/lib/python3.9/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[1;32m    139\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 141\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m           \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m           \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/02514_DeepLearing_CV/dlenv1/lib/python3.9/site-packages/torch/optim/_functional.py:90\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m     87\u001b[0m exp_avg_sq \u001b[38;5;241m=\u001b[39m exp_avg_sqs[i]\n\u001b[1;32m     88\u001b[0m step \u001b[38;5;241m=\u001b[39m state_steps[i]\n\u001b[0;32m---> 90\u001b[0m bias_correction1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43mbeta1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\n\u001b[1;32m     91\u001b[0m bias_correction2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m step\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "crop_images,crop_train_loader, crop_test_loader= train(model,optimizer)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Project 1.1.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
